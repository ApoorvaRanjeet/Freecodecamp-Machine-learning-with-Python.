{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijXNjpK3dXnr"
      },
      "outputs": [],
      "source": [
        "song = \"\"\"So where are you? It's been a little while\n",
        "Sippin' on straight chlorine, let the vibes slide over me\n",
        "This beat is a chemical, beat is a chemical\n",
        "When I leave don't save my seat, I'll be back when it's all complete\n",
        "The moment is medical, moment is medical\n",
        "Sippin' on straight chlorine\n",
        "Lovin' what I'm tastin'\n",
        "Venom on my tongue\n",
        "Dependant at times\n",
        "Poisonous vibration\n",
        "Help my body run\n",
        "I'm runnin' for my li-i-i-i-i-i-fe\n",
        "Runnin' for my li-i-i-i-i-i-fe\n",
        "Sippin' on straight chlorine, let the vibes slide over me\n",
        "This beat is a chemical, beat is a chemical\n",
        "When I leave don't save my seat, I'll be back when it's all complete\n",
        "The moment is medical, moment is medical\n",
        "Sippin' on straight chlorine\n",
        "Fall out of formation\n",
        "I plan my escape from walls they confined\n",
        "Rebel red carnation\n",
        "Grows while I decay\n",
        "I'm runnin' for my li-i-i-i-i-i-fe\n",
        "Runnin' for my li-i-i-i-i-i-fe\n",
        "Yeah, I'm runnin' for my li-i-i-i-i-i-fe\n",
        "Runnin' for my li-i-i-i-i-i-fe\n",
        "Had you in my coat pocket, where I kept my rebel red\n",
        "I felt I was invincible, you wrapped around my head\n",
        "Now different lives I lead, my body lives on lead\n",
        "The last two lines may read incorrect until said\n",
        "The lead is terrible in flavor\n",
        "But now you double as a papermaker\n",
        "I despise you sometimes\n",
        "I love to hate the fight and you in my life is like\n",
        "Sippin' on straight chlorine, let the vibes slide over me\n",
        "This beat is a chemical, beat is a chemical\n",
        "When I leave don't save my seat, I'll be back when it's all complete\n",
        "The moment is medical, moment is medical\n",
        "Sippin' on straight chlorine\n",
        "Let the vibe, let the vibe\n",
        "Let the vibe, let the vibe\n",
        "Beat is a chemical, yeah\n",
        "Let the vibe, let the vibe\n",
        "Let the vibe, let the vibe\n",
        "Moment is medical, yeah\n",
        "Sippin' on straight chlorine\n",
        "Let the vibe, let the vibe\n",
        "Let the vibe, let the vibe\n",
        "Beat is a chemical, yeah\n",
        "Let the vibe, let the vibe\n",
        "Let the vibe, let the vibe\n",
        "Moment is medical, yeah\n",
        "I'm so sorry, I forgot you\n",
        "Let me catch you up to speed\n",
        "I've been tested like the ends of\n",
        "A weathered flag that's by the sea\n",
        "Can you build my house with pieces?\n",
        "I'm just a chemical\n",
        "Can you build my house with pieces?\n",
        "I'm just a chemical\n",
        "Can you build my house with pieces?\n",
        "I'm just a chemical\n",
        "Can you build my house with pieces?\n",
        "I'm just a chemical\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62-4UaY9e08A"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zCpVe-Ge8vY"
      },
      "outputs": [],
      "source": [
        "# creating the object of tokenizer class\n",
        "tokenizer = Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HPrL8VsfFP4"
      },
      "outputs": [],
      "source": [
        "tokenizer.fit_on_texts([song])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75FgVR6BfT3q",
        "outputId": "e31299ab-1387-4948-ef5b-53b3519cd3c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'i': 1,\n",
              " 'the': 2,\n",
              " 'my': 3,\n",
              " 'let': 4,\n",
              " 'is': 5,\n",
              " 'vibe': 6,\n",
              " 'a': 7,\n",
              " 'you': 8,\n",
              " 'chemical': 9,\n",
              " 'on': 10,\n",
              " \"i'm\": 11,\n",
              " 'beat': 12,\n",
              " 'moment': 13,\n",
              " 'medical': 14,\n",
              " \"sippin'\": 15,\n",
              " 'straight': 16,\n",
              " 'chlorine': 17,\n",
              " 'when': 18,\n",
              " \"runnin'\": 19,\n",
              " 'for': 20,\n",
              " 'li': 21,\n",
              " 'fe': 22,\n",
              " 'yeah': 23,\n",
              " \"it's\": 24,\n",
              " 'me': 25,\n",
              " 'can': 26,\n",
              " 'build': 27,\n",
              " 'house': 28,\n",
              " 'with': 29,\n",
              " 'pieces': 30,\n",
              " 'just': 31,\n",
              " 'vibes': 32,\n",
              " 'slide': 33,\n",
              " 'over': 34,\n",
              " 'this': 35,\n",
              " 'leave': 36,\n",
              " \"don't\": 37,\n",
              " 'save': 38,\n",
              " 'seat': 39,\n",
              " \"i'll\": 40,\n",
              " 'be': 41,\n",
              " 'back': 42,\n",
              " 'all': 43,\n",
              " 'complete': 44,\n",
              " 'in': 45,\n",
              " 'lead': 46,\n",
              " 'so': 47,\n",
              " 'where': 48,\n",
              " 'been': 49,\n",
              " 'while': 50,\n",
              " 'body': 51,\n",
              " 'of': 52,\n",
              " 'rebel': 53,\n",
              " 'red': 54,\n",
              " 'now': 55,\n",
              " 'lives': 56,\n",
              " 'to': 57,\n",
              " 'like': 58,\n",
              " 'are': 59,\n",
              " 'little': 60,\n",
              " \"lovin'\": 61,\n",
              " 'what': 62,\n",
              " \"tastin'\": 63,\n",
              " 'venom': 64,\n",
              " 'tongue': 65,\n",
              " 'dependant': 66,\n",
              " 'at': 67,\n",
              " 'times': 68,\n",
              " 'poisonous': 69,\n",
              " 'vibration': 70,\n",
              " 'help': 71,\n",
              " 'run': 72,\n",
              " 'fall': 73,\n",
              " 'out': 74,\n",
              " 'formation': 75,\n",
              " 'plan': 76,\n",
              " 'escape': 77,\n",
              " 'from': 78,\n",
              " 'walls': 79,\n",
              " 'they': 80,\n",
              " 'confined': 81,\n",
              " 'carnation': 82,\n",
              " 'grows': 83,\n",
              " 'decay': 84,\n",
              " 'had': 85,\n",
              " 'coat': 86,\n",
              " 'pocket': 87,\n",
              " 'kept': 88,\n",
              " 'felt': 89,\n",
              " 'was': 90,\n",
              " 'invincible': 91,\n",
              " 'wrapped': 92,\n",
              " 'around': 93,\n",
              " 'head': 94,\n",
              " 'different': 95,\n",
              " 'last': 96,\n",
              " 'two': 97,\n",
              " 'lines': 98,\n",
              " 'may': 99,\n",
              " 'read': 100,\n",
              " 'incorrect': 101,\n",
              " 'until': 102,\n",
              " 'said': 103,\n",
              " 'terrible': 104,\n",
              " 'flavor': 105,\n",
              " 'but': 106,\n",
              " 'double': 107,\n",
              " 'as': 108,\n",
              " 'papermaker': 109,\n",
              " 'despise': 110,\n",
              " 'sometimes': 111,\n",
              " 'love': 112,\n",
              " 'hate': 113,\n",
              " 'fight': 114,\n",
              " 'and': 115,\n",
              " 'life': 116,\n",
              " 'sorry': 117,\n",
              " 'forgot': 118,\n",
              " 'catch': 119,\n",
              " 'up': 120,\n",
              " 'speed': 121,\n",
              " \"i've\": 122,\n",
              " 'tested': 123,\n",
              " 'ends': 124,\n",
              " 'weathered': 125,\n",
              " 'flag': 126,\n",
              " \"that's\": 127,\n",
              " 'by': 128,\n",
              " 'sea': 129}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11TSa89Lfaci"
      },
      "outputs": [],
      "source": [
        "# to check the sentence in our dataset\n",
        "# for sentence in song.split('\\n'):\n",
        "#   print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFHEUOYBgcC2"
      },
      "outputs": [],
      "source": [
        "# now we want to convert these sentence in the numbers\n",
        "# for sentence in song.split('\\n'):\n",
        "#   print(tokenizer.texts_to_sequences([sentence]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "991ylHKNg8UA"
      },
      "outputs": [],
      "source": [
        "# we have to make input and output out of this above data\n",
        "input_sequences = []\n",
        "for sentence in song.split('\\n'):\n",
        "  tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "  for i in range(1,len(tokenized_sentence)):\n",
        "    input_sequences.append(tokenized_sentence[:i+1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEI7sxwHkN4a",
        "outputId": "d371e919-6578-4db0-bc57-ffe71eff3ce4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[47, 48],\n",
              " [47, 48, 59],\n",
              " [47, 48, 59, 8],\n",
              " [47, 48, 59, 8, 24],\n",
              " [47, 48, 59, 8, 24, 49],\n",
              " [47, 48, 59, 8, 24, 49, 7],\n",
              " [47, 48, 59, 8, 24, 49, 7, 60],\n",
              " [47, 48, 59, 8, 24, 49, 7, 60, 50],\n",
              " [15, 10],\n",
              " [15, 10, 16],\n",
              " [15, 10, 16, 17],\n",
              " [15, 10, 16, 17, 4],\n",
              " [15, 10, 16, 17, 4, 2],\n",
              " [15, 10, 16, 17, 4, 2, 32],\n",
              " [15, 10, 16, 17, 4, 2, 32, 33],\n",
              " [15, 10, 16, 17, 4, 2, 32, 33, 34],\n",
              " [15, 10, 16, 17, 4, 2, 32, 33, 34, 25],\n",
              " [35, 12],\n",
              " [35, 12, 5],\n",
              " [35, 12, 5, 7],\n",
              " [35, 12, 5, 7, 9],\n",
              " [35, 12, 5, 7, 9, 12],\n",
              " [35, 12, 5, 7, 9, 12, 5],\n",
              " [35, 12, 5, 7, 9, 12, 5, 7],\n",
              " [35, 12, 5, 7, 9, 12, 5, 7, 9],\n",
              " [18, 1],\n",
              " [18, 1, 36],\n",
              " [18, 1, 36, 37],\n",
              " [18, 1, 36, 37, 38],\n",
              " [18, 1, 36, 37, 38, 3],\n",
              " [18, 1, 36, 37, 38, 3, 39],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41, 42],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41, 42, 18],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41, 42, 18, 24],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41, 42, 18, 24, 43],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41, 42, 18, 24, 43, 44],\n",
              " [2, 13],\n",
              " [2, 13, 5],\n",
              " [2, 13, 5, 14],\n",
              " [2, 13, 5, 14, 13],\n",
              " [2, 13, 5, 14, 13, 5],\n",
              " [2, 13, 5, 14, 13, 5, 14],\n",
              " [15, 10],\n",
              " [15, 10, 16],\n",
              " [15, 10, 16, 17],\n",
              " [61, 62],\n",
              " [61, 62, 11],\n",
              " [61, 62, 11, 63],\n",
              " [64, 10],\n",
              " [64, 10, 3],\n",
              " [64, 10, 3, 65],\n",
              " [66, 67],\n",
              " [66, 67, 68],\n",
              " [69, 70],\n",
              " [71, 3],\n",
              " [71, 3, 51],\n",
              " [71, 3, 51, 72],\n",
              " [11, 19],\n",
              " [11, 19, 20],\n",
              " [11, 19, 20, 3],\n",
              " [11, 19, 20, 3, 21],\n",
              " [11, 19, 20, 3, 21, 1],\n",
              " [11, 19, 20, 3, 21, 1, 1],\n",
              " [11, 19, 20, 3, 21, 1, 1, 1],\n",
              " [11, 19, 20, 3, 21, 1, 1, 1, 1],\n",
              " [11, 19, 20, 3, 21, 1, 1, 1, 1, 1],\n",
              " [11, 19, 20, 3, 21, 1, 1, 1, 1, 1, 22],\n",
              " [19, 20],\n",
              " [19, 20, 3],\n",
              " [19, 20, 3, 21],\n",
              " [19, 20, 3, 21, 1],\n",
              " [19, 20, 3, 21, 1, 1],\n",
              " [19, 20, 3, 21, 1, 1, 1],\n",
              " [19, 20, 3, 21, 1, 1, 1, 1],\n",
              " [19, 20, 3, 21, 1, 1, 1, 1, 1],\n",
              " [19, 20, 3, 21, 1, 1, 1, 1, 1, 22],\n",
              " [15, 10],\n",
              " [15, 10, 16],\n",
              " [15, 10, 16, 17],\n",
              " [15, 10, 16, 17, 4],\n",
              " [15, 10, 16, 17, 4, 2],\n",
              " [15, 10, 16, 17, 4, 2, 32],\n",
              " [15, 10, 16, 17, 4, 2, 32, 33],\n",
              " [15, 10, 16, 17, 4, 2, 32, 33, 34],\n",
              " [15, 10, 16, 17, 4, 2, 32, 33, 34, 25],\n",
              " [35, 12],\n",
              " [35, 12, 5],\n",
              " [35, 12, 5, 7],\n",
              " [35, 12, 5, 7, 9],\n",
              " [35, 12, 5, 7, 9, 12],\n",
              " [35, 12, 5, 7, 9, 12, 5],\n",
              " [35, 12, 5, 7, 9, 12, 5, 7],\n",
              " [35, 12, 5, 7, 9, 12, 5, 7, 9],\n",
              " [18, 1],\n",
              " [18, 1, 36],\n",
              " [18, 1, 36, 37],\n",
              " [18, 1, 36, 37, 38],\n",
              " [18, 1, 36, 37, 38, 3],\n",
              " [18, 1, 36, 37, 38, 3, 39],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41, 42],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41, 42, 18],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41, 42, 18, 24],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41, 42, 18, 24, 43],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41, 42, 18, 24, 43, 44],\n",
              " [2, 13],\n",
              " [2, 13, 5],\n",
              " [2, 13, 5, 14],\n",
              " [2, 13, 5, 14, 13],\n",
              " [2, 13, 5, 14, 13, 5],\n",
              " [2, 13, 5, 14, 13, 5, 14],\n",
              " [15, 10],\n",
              " [15, 10, 16],\n",
              " [15, 10, 16, 17],\n",
              " [73, 74],\n",
              " [73, 74, 52],\n",
              " [73, 74, 52, 75],\n",
              " [1, 76],\n",
              " [1, 76, 3],\n",
              " [1, 76, 3, 77],\n",
              " [1, 76, 3, 77, 78],\n",
              " [1, 76, 3, 77, 78, 79],\n",
              " [1, 76, 3, 77, 78, 79, 80],\n",
              " [1, 76, 3, 77, 78, 79, 80, 81],\n",
              " [53, 54],\n",
              " [53, 54, 82],\n",
              " [83, 50],\n",
              " [83, 50, 1],\n",
              " [83, 50, 1, 84],\n",
              " [11, 19],\n",
              " [11, 19, 20],\n",
              " [11, 19, 20, 3],\n",
              " [11, 19, 20, 3, 21],\n",
              " [11, 19, 20, 3, 21, 1],\n",
              " [11, 19, 20, 3, 21, 1, 1],\n",
              " [11, 19, 20, 3, 21, 1, 1, 1],\n",
              " [11, 19, 20, 3, 21, 1, 1, 1, 1],\n",
              " [11, 19, 20, 3, 21, 1, 1, 1, 1, 1],\n",
              " [11, 19, 20, 3, 21, 1, 1, 1, 1, 1, 22],\n",
              " [19, 20],\n",
              " [19, 20, 3],\n",
              " [19, 20, 3, 21],\n",
              " [19, 20, 3, 21, 1],\n",
              " [19, 20, 3, 21, 1, 1],\n",
              " [19, 20, 3, 21, 1, 1, 1],\n",
              " [19, 20, 3, 21, 1, 1, 1, 1],\n",
              " [19, 20, 3, 21, 1, 1, 1, 1, 1],\n",
              " [19, 20, 3, 21, 1, 1, 1, 1, 1, 22],\n",
              " [23, 11],\n",
              " [23, 11, 19],\n",
              " [23, 11, 19, 20],\n",
              " [23, 11, 19, 20, 3],\n",
              " [23, 11, 19, 20, 3, 21],\n",
              " [23, 11, 19, 20, 3, 21, 1],\n",
              " [23, 11, 19, 20, 3, 21, 1, 1],\n",
              " [23, 11, 19, 20, 3, 21, 1, 1, 1],\n",
              " [23, 11, 19, 20, 3, 21, 1, 1, 1, 1],\n",
              " [23, 11, 19, 20, 3, 21, 1, 1, 1, 1, 1],\n",
              " [23, 11, 19, 20, 3, 21, 1, 1, 1, 1, 1, 22],\n",
              " [19, 20],\n",
              " [19, 20, 3],\n",
              " [19, 20, 3, 21],\n",
              " [19, 20, 3, 21, 1],\n",
              " [19, 20, 3, 21, 1, 1],\n",
              " [19, 20, 3, 21, 1, 1, 1],\n",
              " [19, 20, 3, 21, 1, 1, 1, 1],\n",
              " [19, 20, 3, 21, 1, 1, 1, 1, 1],\n",
              " [19, 20, 3, 21, 1, 1, 1, 1, 1, 22],\n",
              " [85, 8],\n",
              " [85, 8, 45],\n",
              " [85, 8, 45, 3],\n",
              " [85, 8, 45, 3, 86],\n",
              " [85, 8, 45, 3, 86, 87],\n",
              " [85, 8, 45, 3, 86, 87, 48],\n",
              " [85, 8, 45, 3, 86, 87, 48, 1],\n",
              " [85, 8, 45, 3, 86, 87, 48, 1, 88],\n",
              " [85, 8, 45, 3, 86, 87, 48, 1, 88, 3],\n",
              " [85, 8, 45, 3, 86, 87, 48, 1, 88, 3, 53],\n",
              " [85, 8, 45, 3, 86, 87, 48, 1, 88, 3, 53, 54],\n",
              " [1, 89],\n",
              " [1, 89, 1],\n",
              " [1, 89, 1, 90],\n",
              " [1, 89, 1, 90, 91],\n",
              " [1, 89, 1, 90, 91, 8],\n",
              " [1, 89, 1, 90, 91, 8, 92],\n",
              " [1, 89, 1, 90, 91, 8, 92, 93],\n",
              " [1, 89, 1, 90, 91, 8, 92, 93, 3],\n",
              " [1, 89, 1, 90, 91, 8, 92, 93, 3, 94],\n",
              " [55, 95],\n",
              " [55, 95, 56],\n",
              " [55, 95, 56, 1],\n",
              " [55, 95, 56, 1, 46],\n",
              " [55, 95, 56, 1, 46, 3],\n",
              " [55, 95, 56, 1, 46, 3, 51],\n",
              " [55, 95, 56, 1, 46, 3, 51, 56],\n",
              " [55, 95, 56, 1, 46, 3, 51, 56, 10],\n",
              " [55, 95, 56, 1, 46, 3, 51, 56, 10, 46],\n",
              " [2, 96],\n",
              " [2, 96, 97],\n",
              " [2, 96, 97, 98],\n",
              " [2, 96, 97, 98, 99],\n",
              " [2, 96, 97, 98, 99, 100],\n",
              " [2, 96, 97, 98, 99, 100, 101],\n",
              " [2, 96, 97, 98, 99, 100, 101, 102],\n",
              " [2, 96, 97, 98, 99, 100, 101, 102, 103],\n",
              " [2, 46],\n",
              " [2, 46, 5],\n",
              " [2, 46, 5, 104],\n",
              " [2, 46, 5, 104, 45],\n",
              " [2, 46, 5, 104, 45, 105],\n",
              " [106, 55],\n",
              " [106, 55, 8],\n",
              " [106, 55, 8, 107],\n",
              " [106, 55, 8, 107, 108],\n",
              " [106, 55, 8, 107, 108, 7],\n",
              " [106, 55, 8, 107, 108, 7, 109],\n",
              " [1, 110],\n",
              " [1, 110, 8],\n",
              " [1, 110, 8, 111],\n",
              " [1, 112],\n",
              " [1, 112, 57],\n",
              " [1, 112, 57, 113],\n",
              " [1, 112, 57, 113, 2],\n",
              " [1, 112, 57, 113, 2, 114],\n",
              " [1, 112, 57, 113, 2, 114, 115],\n",
              " [1, 112, 57, 113, 2, 114, 115, 8],\n",
              " [1, 112, 57, 113, 2, 114, 115, 8, 45],\n",
              " [1, 112, 57, 113, 2, 114, 115, 8, 45, 3],\n",
              " [1, 112, 57, 113, 2, 114, 115, 8, 45, 3, 116],\n",
              " [1, 112, 57, 113, 2, 114, 115, 8, 45, 3, 116, 5],\n",
              " [1, 112, 57, 113, 2, 114, 115, 8, 45, 3, 116, 5, 58],\n",
              " [15, 10],\n",
              " [15, 10, 16],\n",
              " [15, 10, 16, 17],\n",
              " [15, 10, 16, 17, 4],\n",
              " [15, 10, 16, 17, 4, 2],\n",
              " [15, 10, 16, 17, 4, 2, 32],\n",
              " [15, 10, 16, 17, 4, 2, 32, 33],\n",
              " [15, 10, 16, 17, 4, 2, 32, 33, 34],\n",
              " [15, 10, 16, 17, 4, 2, 32, 33, 34, 25],\n",
              " [35, 12],\n",
              " [35, 12, 5],\n",
              " [35, 12, 5, 7],\n",
              " [35, 12, 5, 7, 9],\n",
              " [35, 12, 5, 7, 9, 12],\n",
              " [35, 12, 5, 7, 9, 12, 5],\n",
              " [35, 12, 5, 7, 9, 12, 5, 7],\n",
              " [35, 12, 5, 7, 9, 12, 5, 7, 9],\n",
              " [18, 1],\n",
              " [18, 1, 36],\n",
              " [18, 1, 36, 37],\n",
              " [18, 1, 36, 37, 38],\n",
              " [18, 1, 36, 37, 38, 3],\n",
              " [18, 1, 36, 37, 38, 3, 39],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41, 42],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41, 42, 18],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41, 42, 18, 24],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41, 42, 18, 24, 43],\n",
              " [18, 1, 36, 37, 38, 3, 39, 40, 41, 42, 18, 24, 43, 44],\n",
              " [2, 13],\n",
              " [2, 13, 5],\n",
              " [2, 13, 5, 14],\n",
              " [2, 13, 5, 14, 13],\n",
              " [2, 13, 5, 14, 13, 5],\n",
              " [2, 13, 5, 14, 13, 5, 14],\n",
              " [15, 10],\n",
              " [15, 10, 16],\n",
              " [15, 10, 16, 17],\n",
              " [4, 2],\n",
              " [4, 2, 6],\n",
              " [4, 2, 6, 4],\n",
              " [4, 2, 6, 4, 2],\n",
              " [4, 2, 6, 4, 2, 6],\n",
              " [4, 2],\n",
              " [4, 2, 6],\n",
              " [4, 2, 6, 4],\n",
              " [4, 2, 6, 4, 2],\n",
              " [4, 2, 6, 4, 2, 6],\n",
              " [12, 5],\n",
              " [12, 5, 7],\n",
              " [12, 5, 7, 9],\n",
              " [12, 5, 7, 9, 23],\n",
              " [4, 2],\n",
              " [4, 2, 6],\n",
              " [4, 2, 6, 4],\n",
              " [4, 2, 6, 4, 2],\n",
              " [4, 2, 6, 4, 2, 6],\n",
              " [4, 2],\n",
              " [4, 2, 6],\n",
              " [4, 2, 6, 4],\n",
              " [4, 2, 6, 4, 2],\n",
              " [4, 2, 6, 4, 2, 6],\n",
              " [13, 5],\n",
              " [13, 5, 14],\n",
              " [13, 5, 14, 23],\n",
              " [15, 10],\n",
              " [15, 10, 16],\n",
              " [15, 10, 16, 17],\n",
              " [4, 2],\n",
              " [4, 2, 6],\n",
              " [4, 2, 6, 4],\n",
              " [4, 2, 6, 4, 2],\n",
              " [4, 2, 6, 4, 2, 6],\n",
              " [4, 2],\n",
              " [4, 2, 6],\n",
              " [4, 2, 6, 4],\n",
              " [4, 2, 6, 4, 2],\n",
              " [4, 2, 6, 4, 2, 6],\n",
              " [12, 5],\n",
              " [12, 5, 7],\n",
              " [12, 5, 7, 9],\n",
              " [12, 5, 7, 9, 23],\n",
              " [4, 2],\n",
              " [4, 2, 6],\n",
              " [4, 2, 6, 4],\n",
              " [4, 2, 6, 4, 2],\n",
              " [4, 2, 6, 4, 2, 6],\n",
              " [4, 2],\n",
              " [4, 2, 6],\n",
              " [4, 2, 6, 4],\n",
              " [4, 2, 6, 4, 2],\n",
              " [4, 2, 6, 4, 2, 6],\n",
              " [13, 5],\n",
              " [13, 5, 14],\n",
              " [13, 5, 14, 23],\n",
              " [11, 47],\n",
              " [11, 47, 117],\n",
              " [11, 47, 117, 1],\n",
              " [11, 47, 117, 1, 118],\n",
              " [11, 47, 117, 1, 118, 8],\n",
              " [4, 25],\n",
              " [4, 25, 119],\n",
              " [4, 25, 119, 8],\n",
              " [4, 25, 119, 8, 120],\n",
              " [4, 25, 119, 8, 120, 57],\n",
              " [4, 25, 119, 8, 120, 57, 121],\n",
              " [122, 49],\n",
              " [122, 49, 123],\n",
              " [122, 49, 123, 58],\n",
              " [122, 49, 123, 58, 2],\n",
              " [122, 49, 123, 58, 2, 124],\n",
              " [122, 49, 123, 58, 2, 124, 52],\n",
              " [7, 125],\n",
              " [7, 125, 126],\n",
              " [7, 125, 126, 127],\n",
              " [7, 125, 126, 127, 128],\n",
              " [7, 125, 126, 127, 128, 2],\n",
              " [7, 125, 126, 127, 128, 2, 129],\n",
              " [26, 8],\n",
              " [26, 8, 27],\n",
              " [26, 8, 27, 3],\n",
              " [26, 8, 27, 3, 28],\n",
              " [26, 8, 27, 3, 28, 29],\n",
              " [26, 8, 27, 3, 28, 29, 30],\n",
              " [11, 31],\n",
              " [11, 31, 7],\n",
              " [11, 31, 7, 9],\n",
              " [26, 8],\n",
              " [26, 8, 27],\n",
              " [26, 8, 27, 3],\n",
              " [26, 8, 27, 3, 28],\n",
              " [26, 8, 27, 3, 28, 29],\n",
              " [26, 8, 27, 3, 28, 29, 30],\n",
              " [11, 31],\n",
              " [11, 31, 7],\n",
              " [11, 31, 7, 9],\n",
              " [26, 8],\n",
              " [26, 8, 27],\n",
              " [26, 8, 27, 3],\n",
              " [26, 8, 27, 3, 28],\n",
              " [26, 8, 27, 3, 28, 29],\n",
              " [26, 8, 27, 3, 28, 29, 30],\n",
              " [11, 31],\n",
              " [11, 31, 7],\n",
              " [11, 31, 7, 9],\n",
              " [26, 8],\n",
              " [26, 8, 27],\n",
              " [26, 8, 27, 3],\n",
              " [26, 8, 27, 3, 28],\n",
              " [26, 8, 27, 3, 28, 29],\n",
              " [26, 8, 27, 3, 28, 29, 30],\n",
              " [11, 31],\n",
              " [11, 31, 7],\n",
              " [11, 31, 7, 9]]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "edtjsNN-oDkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0847059-61e9-4d75-cdb1-14e85fbe30b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        }
      ],
      "source": [
        "# all the sentences are of uneven length , we need to make them all of equal length\n",
        "# here we are finding the length of each sentence and putting them in the list\n",
        "\n",
        "max_len = max([len(x) for x in input_sequences])\n",
        "print(max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaiCQB8yo4P3"
      },
      "outputs": [],
      "source": [
        "# now we will do zero padding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "pad_input_sequence = pad_sequences(input_sequences,maxlen=max_len,padding='pre')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPKIdSByqYcH"
      },
      "outputs": [],
      "source": [
        "X = pad_input_sequence[:,:-1] # extract all the rows except for the last column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOsSsTVNsEAx"
      },
      "outputs": [],
      "source": [
        "Y = pad_input_sequence[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Lr97NyYsXK9"
      },
      "outputs": [],
      "source": [
        "# now we are going to transform our output by one-hot encoding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "Y = to_categorical(Y,num_classes=130)  # in num_classes we specify the total number of words\n",
        "# we gave 130 instead of 129 coz tokinezer initialized the words from 1 but one-hot encoding starts with 0 , so if we give 129 then it will encode upto 128 words 129th word will not be encoded as it has started from 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsakXedq4JCW",
        "outputId": "6c762d00-de4d-4949-c9b4-f491a388706d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(389, 13)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwov2y6F0A1W",
        "outputId": "3561dc93-8c97-41ab-e9a3-6fc4d732ba69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(389, 130)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bALeww2q0QeR",
        "outputId": "a777ccf3-fdb2-49aa-d7cb-dbe698074d49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1jAHTz60YTl"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Embedding,LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kropZhJP1wv6"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(130,100,input_length=13)) # total words,dense nodes,number of words in each sentence\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(130,activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRGLWAvi4oUg"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',# coz its multiclass classification\n",
        "              optimizer = 'adam',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b2ZtlgB5Dcp",
        "outputId": "b1ab7c78-7e06-4fea-e782-f3f7d066f7b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 13, 100)           13000     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 150)               150600    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 130)               19630     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 183230 (715.74 KB)\n",
            "Trainable params: 183230 (715.74 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE9tUezEseU3",
        "outputId": "df64cf2c-8613-4227-8536-4aea0aca502c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "13/13 [==============================] - 3s 36ms/step - loss: 4.8269 - accuracy: 0.1054\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 4.3871 - accuracy: 0.1028\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 4.1613 - accuracy: 0.0977\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 0s 33ms/step - loss: 4.0829 - accuracy: 0.0977\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 3.9741 - accuracy: 0.1054\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 3.8158 - accuracy: 0.1697\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 3.6823 - accuracy: 0.1465\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 3.4940 - accuracy: 0.2314\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 3.3151 - accuracy: 0.2545\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 3.1527 - accuracy: 0.2571\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 3.0000 - accuracy: 0.3033\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 1s 59ms/step - loss: 2.8490 - accuracy: 0.3522\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 1s 60ms/step - loss: 2.6734 - accuracy: 0.3830\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 1s 58ms/step - loss: 2.5064 - accuracy: 0.4781\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 2.3647 - accuracy: 0.5116\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 2.2234 - accuracy: 0.5553\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 2.0827 - accuracy: 0.5604\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 1.9668 - accuracy: 0.6195\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 1.8488 - accuracy: 0.6375\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 1.7184 - accuracy: 0.6478\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 1.6057 - accuracy: 0.6941\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 1.5148 - accuracy: 0.7147\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 1.4208 - accuracy: 0.7275\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 1.3549 - accuracy: 0.7301\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 1.2566 - accuracy: 0.7404\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 1.1882 - accuracy: 0.7609\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 1.1013 - accuracy: 0.7763\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 1.0341 - accuracy: 0.7892\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 0.9647 - accuracy: 0.8123\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.9085 - accuracy: 0.8226\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 0.8670 - accuracy: 0.8252\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.8234 - accuracy: 0.8303\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.7728 - accuracy: 0.8560\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.7592 - accuracy: 0.8612\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.7130 - accuracy: 0.8663\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.6713 - accuracy: 0.8869\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.6340 - accuracy: 0.8895\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 0.6001 - accuracy: 0.9049\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 0s 28ms/step - loss: 0.5770 - accuracy: 0.9075\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 1s 58ms/step - loss: 0.5429 - accuracy: 0.9152\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 1s 57ms/step - loss: 0.5182 - accuracy: 0.9126\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 1s 57ms/step - loss: 0.4912 - accuracy: 0.9152\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 1s 60ms/step - loss: 0.4655 - accuracy: 0.9229\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 1s 39ms/step - loss: 0.4453 - accuracy: 0.9254\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.4209 - accuracy: 0.9229\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.4190 - accuracy: 0.9229\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.3909 - accuracy: 0.9332\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.3693 - accuracy: 0.9460\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 1s 40ms/step - loss: 0.3521 - accuracy: 0.9486\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.3368 - accuracy: 0.9563\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.3229 - accuracy: 0.9589\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 0.3116 - accuracy: 0.9563\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.2977 - accuracy: 0.9589\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 0.2876 - accuracy: 0.9614\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.2762 - accuracy: 0.9666\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.2894 - accuracy: 0.9563\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.2781 - accuracy: 0.9589\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 0.2490 - accuracy: 0.9666\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.2364 - accuracy: 0.9717\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 0.2289 - accuracy: 0.9666\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.2177 - accuracy: 0.9692\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 0s 34ms/step - loss: 0.2092 - accuracy: 0.9743\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 0.2031 - accuracy: 0.9717\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.1966 - accuracy: 0.9717\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.1926 - accuracy: 0.9769\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.1854 - accuracy: 0.9769\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.1808 - accuracy: 0.9743\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.1744 - accuracy: 0.9717\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 1s 60ms/step - loss: 0.1705 - accuracy: 0.9717\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 1s 61ms/step - loss: 0.1731 - accuracy: 0.9769\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 1s 60ms/step - loss: 0.1611 - accuracy: 0.9769\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 0.1541 - accuracy: 0.9769\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.1512 - accuracy: 0.9769\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 0.1485 - accuracy: 0.9769\n",
            "Epoch 75/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.1424 - accuracy: 0.9769\n",
            "Epoch 76/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.1398 - accuracy: 0.9769\n",
            "Epoch 77/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 0.1342 - accuracy: 0.9769\n",
            "Epoch 78/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 0.1302 - accuracy: 0.9769\n",
            "Epoch 79/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.1290 - accuracy: 0.9743\n",
            "Epoch 80/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.1247 - accuracy: 0.9769\n",
            "Epoch 81/100\n",
            "13/13 [==============================] - 0s 33ms/step - loss: 0.1299 - accuracy: 0.9743\n",
            "Epoch 82/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.1262 - accuracy: 0.9743\n",
            "Epoch 83/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.1175 - accuracy: 0.9743\n",
            "Epoch 84/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.1165 - accuracy: 0.9743\n",
            "Epoch 85/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.1153 - accuracy: 0.9743\n",
            "Epoch 86/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.1132 - accuracy: 0.9743\n",
            "Epoch 87/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.1120 - accuracy: 0.9717\n",
            "Epoch 88/100\n",
            "13/13 [==============================] - 0s 35ms/step - loss: 0.1088 - accuracy: 0.9743\n",
            "Epoch 89/100\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 0.1126 - accuracy: 0.9717\n",
            "Epoch 90/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.1109 - accuracy: 0.9717\n",
            "Epoch 91/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 0.1053 - accuracy: 0.9743\n",
            "Epoch 92/100\n",
            "13/13 [==============================] - 0s 28ms/step - loss: 0.1025 - accuracy: 0.9743\n",
            "Epoch 93/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.1022 - accuracy: 0.9769\n",
            "Epoch 94/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 0.1089 - accuracy: 0.9692\n",
            "Epoch 95/100\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 0.1045 - accuracy: 0.9717\n",
            "Epoch 96/100\n",
            "13/13 [==============================] - 0s 36ms/step - loss: 0.0984 - accuracy: 0.9769\n",
            "Epoch 97/100\n",
            "13/13 [==============================] - 1s 57ms/step - loss: 0.0966 - accuracy: 0.9769\n",
            "Epoch 98/100\n",
            "13/13 [==============================] - 1s 60ms/step - loss: 0.0995 - accuracy: 0.9717\n",
            "Epoch 99/100\n",
            "13/13 [==============================] - 1s 58ms/step - loss: 0.0953 - accuracy: 0.9717\n",
            "Epoch 100/100\n",
            "13/13 [==============================] - 1s 59ms/step - loss: 0.0986 - accuracy: 0.9717\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78a1744b4a90>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X,Y,epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction"
      ],
      "metadata": {
        "id": "u3JDxPp_tr8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "i5Gcu_Bywgij"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "i_1682Jmsiz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "362d996a-1f8a-4daf-93dc-9c20b19e53f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Hi This is Apoorva a chemical beat is a chemical beat is a chemical\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"Hi This is Apoorva\"\"\"\n",
        "for i in range(10):\n",
        "  # tokenize\n",
        "  token_text = tokenizer.texts_to_sequences([text])[0] # we put zero to get list not nested list\n",
        "  #padding\n",
        "  padded_token_input = pad_sequences([token_text],maxlen=13,padding='pre')\n",
        "  #predict\n",
        "  # model.predict(padded_token_input).shape\n",
        "\n",
        "  pos = np.argmax(model.predict(padded_token_input))\n",
        "\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == pos:\n",
        "      text = text + \" \" + word\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X_AR6zEVxSKL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}